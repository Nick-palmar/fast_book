## NLP Deep Dive: RNNs

1. Self-supervised learning means not giving labels to a model - just giving it lots and lots of data and the model learning from the data by creating labels automatically. In NLP applications such as language models, models are often given lots of words and the labels are simply the next word after the current word. The labels are embedded in the independant variable. 

2. A language model is a model trained to guess the next word in text after reading the previous ones. 

3. A language model is considered self-supervised because it takes lots and lots of text data and creates the labels by offsetting the words by 1 in the independent variable. This way, the labels are always the next word in the sentence which are being predicted. 

4. Self-supervised learning is often used for pretraining of models using for transfer learning - it is not normally the 'final' model. 

5. Language models are fine tuned to a particular copus before adding a classifer layer at the end because the ULMFiT paper approach proved that fine tuning the langauge model prior to fine tuning the classificaiton model (end result) yielded better results. Intuatively, fine tuning the language model on a particular corpus makes the model understand the specific type of language better before converting it into a classifier of some type - as such, it has learned about the terminology associated with the classification task and will be more equipped to complete the classification task well. 

6. To create a state-of-the-art text classifier, one can follow the ULMFiT approach which has three steps: first training a language model on a large dataset (wikitext 103), then training the model on context specific data, and finally training a classifier with the context specific data. Jeremy Howard shows how this novel idea to nlp transfer learning is very effective with the addition of disciminative fine tuning, slanted learning rates. 

7. The 50,000 unlabelled movie reviews help create a better text classifier as they can be used to train the langauge model in the 2nd training step of ULMFiT (by training the language model on the 50,000 unlabelled movie reviews). As such, the language model will become familiar with the movie review corpus and generalize better as a classifier. The model will be very good at predicting the next words in a movie review. 

8. The three steps to prepare data for a language model are tokenization, numericalization, and language model data laoder creation. 

9. Tokenization is the process of converting the text into a list of words/characters/substrings. Tokenization is required as words can be split in different ways (hyphens, apostrophes) which tokenizers are able to handle. This step is crutial as words must be tokenized in order to be passed into the model as individual tokens and have a specific embedding generated for that token in the embedding matrix (full chunks of text cannot be passed to a model). 

10. Three approached to tokenization are word, characters, and substring tokenization. In general, word tokenization requires a larger vocab and fewer tokens per sentence so in general easier training (faster, less memory, less state). However to learn, it requires more memory/data for large embedding matrix. Character tozenization has less tokens to smaller embedding matrix to learn and more data in terms of tokens per sentence, but will take longer to train and must learn about things like spelling in english so is more difficult to train. Substring tokenization is in the middle, depending on subword vocab size.   

11. xxbos is a special token generated by fastai meaning the beginning of a stream. This is meant to translate english into a sequence that is easy for the model to learn. 

12. 4 rules fastai applies during text tokenization is replace repeated characters/words, removes multiple spaces (only single space), lowercase a capitalized word and adds a new special token, lowercases all text and adds a speical token for beginning/end of stream. 

13. Repeated characters are replaced with a token showing # of reps and the rep character so that the model can encode infomation about repeated characters in the embedding matrix rather than seeing the same embedding multiple times repeated. 

14. Numericalization is the process of mapping tokens to integers. This is done by first creating a vocab - a list of all possible words and then replacing the tokens/words with their index in this vocab. 

15. Some tokens may be replaced by "unknown word" token because the default max_vocab for the fastai numericalize method is 60,000 words (most common 60,000 words). The less commons words may show as unknown characters to prevent having huge embedding matrices and also preventing rare words from being 'learned' (since the rare words do not have enough data to be 'learned'). 

16. Note that the batch size is 64 in this case, and the sequence length for each row in the batch is also 64 tokens. The first row of the first batch contains the first 64 tokens of the first ministream (tokens 1-64). The second row of the first batch contains the beginnning of the second ministream (we do not know which tokens because we do not know the length of an entire row in the batch; this would be calculated by n_tokens/batch_size). The first row of the second batch contains the next 64 tokens of the first ministream (tokens 65-128).  

17. 